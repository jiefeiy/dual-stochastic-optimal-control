{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28bb37a2-eaa3-4cdb-be6d-933106ac79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jacfwd, jacrev, jit, lax, vmap, grad\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c61ef2-89e9-44a8-8eb9-a3eca4876ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, num_layers=1, batch_norm=False):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.fc_in = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act = nn.Tanh()\n",
    "        if batch_norm: self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        else: self.bn = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.bn(self.fc_in(x)))\n",
    "        for layer in self.layers: out = self.act(self.bn(layer(out)))\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "def load_model_from_torch(d):\n",
    "    z_net = MLP2(d+1, d)\n",
    "    z_net.load_state_dict(torch.load(f'./trained_bsde_model_lq/d{d}_z_net_1.pth'))\n",
    "    params_pt = {n: p.detach().numpy() for n, p in z_net.state_dict().items()}\n",
    "    params_jax = {\n",
    "        \"w1\": jnp.array(params_pt[\"fc_in.weight\"]), \"b1\": jnp.array(params_pt[\"fc_in.bias\"]),\n",
    "        \"w2\": jnp.array(params_pt[\"layers.0.weight\"]), \"b2\": jnp.array(params_pt[\"layers.0.bias\"]),\n",
    "        \"w3\": jnp.array(params_pt[\"fc_out.weight\"]), \"b3\": jnp.array(params_pt[\"fc_out.bias\"]),\n",
    "    }   # Convert to JAX params\n",
    "    return params_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7873fba0-24fd-4900-bb91-87c29df218fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_net_jax(params, x):\n",
    "    h1 = jnp.tanh(jnp.dot(x, params[\"w1\"].T) + params[\"b1\"])\n",
    "    h2 = jnp.tanh(jnp.dot(h1, params[\"w2\"].T) + params[\"b2\"])\n",
    "    out = jnp.dot(h2, params[\"w3\"].T) + params[\"b3\"]\n",
    "    return out\n",
    "\n",
    "def div_z(params, tx):\n",
    "    return jnp.trace(jacfwd(lambda x: z_net_jax(params, x))(tx)[:, 1:])  # Calculates divergence: trace of Jacobian w.r.t x (indices 1:)\n",
    "\n",
    "@jit\n",
    "def get_dH_dx_and_div(params, t, x, dw_t):\n",
    "    \"\"\"Computes dH/dx and divergence efficiently.\"\"\"\n",
    "    tx = jnp.concatenate([t[None], x])\n",
    "    \n",
    "    J_z = jacfwd(lambda y: z_net_jax(params, y))(tx)          # Calculate Z and Jacobian of Z w.r.t tx\n",
    "    z_val = z_net_jax(params, tx)\n",
    "    \n",
    "    jx_z = J_z[:, 1:]                                         # Extract Jacobian w.r.t x (d_z/d_x)\n",
    "    div_val = jnp.trace(jx_z)                                 # Divergence is trace of d_z/d_x\n",
    "    \n",
    "    term1 = dw_t @ jx_z\n",
    "    grad_div = jacrev(lambda y: div_z(params, y))(tx)[1:]     # Gradient of divergence: This is a second derivative.\n",
    "    \n",
    "    dh_dx = -term1 + (jnp.sqrt(2.0)/2.0) * grad_div\n",
    "    return dh_dx, z_val, div_val\n",
    "\n",
    "def b_tilde(x, p, dw_t):\n",
    "    return -2*p + jnp.sqrt(2) * dw_t \n",
    "\n",
    "def bm_kl_basis(t_grid, n_terms):\n",
    "    \"\"\"Precompute cosine basis for KL expansion.\"\"\"\n",
    "    js_plus = jnp.arange(0, n_terms) + 0.5\n",
    "    # Shape: (N+1, n_terms)\n",
    "    basis = jnp.sqrt(2) * jnp.cos(jnp.pi * jnp.outer(t_grid, js_plus)) \n",
    "    return basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fccd3fc-8bb5-4176-a39b-607dbd88006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_dot_at_t(t, xi):\n",
    "    \"\"\"\n",
    "    Computes the value of the noise time-derivative at an arbitrary time t.\n",
    "    This allows RK4 to query noise at t + dt/2.\n",
    "    \"\"\"\n",
    "    n_terms = xi.shape[0]\n",
    "    # j_plus = j + 0.5\n",
    "    j_plus = jnp.arange(n_terms) + 0.5\n",
    "    # basis = sqrt(2) * cos(pi * t * (j+0.5))\n",
    "    basis = jnp.sqrt(2) * jnp.cos(jnp.pi * t * j_plus)\n",
    "    return jnp.dot(basis, xi)\n",
    "\n",
    "def get_system_derivs(t, state, xi, params):\n",
    "    \"\"\"\n",
    "    Returns [dx/dt, dp/dt, dCost/dt] given current state (x, p, cost).\n",
    "    \"\"\"\n",
    "    x, p, _ = state  # We don't need the current 'accumulated cost' to calculate derivatives\n",
    "    \n",
    "    dw_t = get_w_dot_at_t(t, xi)\n",
    "    tx = jnp.concatenate([t[None], x])\n",
    "    \n",
    "    # Forward Jacobian of Z w.r.t tx\n",
    "    J_z = jax.jacfwd(lambda y: z_net_jax(params, y))(tx)\n",
    "    z_val = z_net_jax(params, tx)\n",
    "    jx_z = J_z[:, 1:] \n",
    "    div_val = jnp.trace(jx_z)\n",
    "    \n",
    "    grad_div = jax.jacrev(lambda y: div_z(params, y))(tx)[1:]    # Gradient of divergence (using jacrev)\n",
    "    \n",
    "    term1 = dw_t @ jx_z\n",
    "    dh_dx = -term1 + (jnp.sqrt(2.0)/2.0) * grad_div\n",
    "    \n",
    "    dx_dt = -2*p + jnp.sqrt(2) * dw_t     # dx/dt = b_tilde\n",
    "    dp_dt = -dh_dx\n",
    "   \n",
    "    h_part1 = -jnp.sum(p**2) + jnp.sqrt(2) * jnp.dot(p, dw_t)\n",
    "    h_part2 = -jnp.dot(z_val, dw_t) + (jnp.sqrt(2)/2.0) * div_val\n",
    "    H_val = h_part1 + h_part2\n",
    "    \n",
    "    dcost_dt = H_val - jnp.dot(x, dh_dx)\n",
    "    \n",
    "    return dx_dt, dp_dt, dcost_dt\n",
    "\n",
    "# ---- RK4 ----\n",
    "def rk4_step(state, t, dt, xi, params):\n",
    "    \"\"\"\n",
    "    Performs one RK4 step for the coupled system (x, p, cost).\n",
    "    \"\"\"\n",
    "    # k1\n",
    "    k1_x, k1_p, k1_c = get_system_derivs(t, state, xi, params)\n",
    "    \n",
    "    # k2 (at t + dt/2)\n",
    "    state_k2 = (state[0] + 0.5*dt*k1_x, state[1] + 0.5*dt*k1_p, state[2] + 0.5*dt*k1_c)\n",
    "    k2_x, k2_p, k2_c = get_system_derivs(t + 0.5*dt, state_k2, xi, params)\n",
    "    \n",
    "    # k3 (at t + dt/2)\n",
    "    state_k3 = (state[0] + 0.5*dt*k2_x, state[1] + 0.5*dt*k2_p, state[2] + 0.5*dt*k2_c)\n",
    "    k3_x, k3_p, k3_c = get_system_derivs(t + 0.5*dt, state_k3, xi, params)\n",
    "    \n",
    "    # k4 (at t + dt)\n",
    "    state_k4 = (state[0] + dt*k3_x, state[1] + dt*k3_p, state[2] + dt*k3_c)\n",
    "    k4_x, k4_p, k4_c = get_system_derivs(t + dt, state_k4, xi, params)\n",
    "    \n",
    "    # Combine\n",
    "    x_next = state[0] + (dt/6.0)*(k1_x + 2*k2_x + 2*k3_x + k4_x)\n",
    "    p_next = state[1] + (dt/6.0)*(k1_p + 2*k2_p + 2*k3_p + k4_p)\n",
    "    c_next = state[2] + (dt/6.0)*(k1_c + 2*k2_c + 2*k3_c + k4_c)\n",
    "    \n",
    "    return (x_next, p_next, c_next)\n",
    "\n",
    "def loss_fn_single_rk4(p0, xi, params, t_grid, dt):\n",
    "    \"\"\"\n",
    "    Computes objective using RK4 solver.\n",
    "    \"\"\"\n",
    "    d = p0.shape[0]\n",
    "    x0 = jnp.zeros(d)\n",
    "    \n",
    "    init_state = (x0, p0, 0.0)\n",
    "    \n",
    "    times = t_grid[:-1] # 0 to T-dt\n",
    "    \n",
    "    def scan_fn(state, t):\n",
    "        next_state = rk4_step(state, t, dt, xi, params)\n",
    "        return next_state, None\n",
    "\n",
    "    final_state, _ = lax.scan(scan_fn, init_state, times)\n",
    "    \n",
    "    x_T, p_T, total_integral = final_state\n",
    "    \n",
    "    # Terminal Cost (Same as original)\n",
    "    param_g = jnp.array([1.0, 25.0/4.0])\n",
    "    g_star = 0.5 * (jnp.dot(p_T * param_g, p_T) - 1)\n",
    "    \n",
    "    hopf_val = jnp.dot(x0, p0) - g_star + total_integral\n",
    "    return -hopf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff5ccce-6ddf-4712-8693-b88dee434a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Gradient Ascent ---- \n",
    "@jit\n",
    "def optimize_single_sample_rk4(xi, params, t_grid, dt):\n",
    "    d = xi.shape[1]\n",
    "    p0 = jnp.zeros(d)\n",
    "    learning_rate = 0.1\n",
    "    iterations = 100\n",
    "    \n",
    "    val_and_grad_fn = jax.value_and_grad(loss_fn_single_rk4, argnums=0)\n",
    "    \n",
    "    def train_step(state, _):\n",
    "        p, opt_state = state\n",
    "        loss, grads = val_and_grad_fn(p, xi, params, t_grid, dt)\n",
    "        p_new = p - learning_rate * grads\n",
    "        return (p_new, opt_state), loss\n",
    "\n",
    "    (p_final, _), losses = lax.scan(train_step, (p0, 0), None, length=iterations)\n",
    "    return -losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a13aace-40c7-4a08-81ff-2f6a6c101486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling and running for 500 samples...\n",
      "Total Time: 317.9576s\n",
      "------------------------------\n",
      "Dual lower bound: 1.1889 +/- 0.0016\n",
      "95% lower confidence limit: 1.1873\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def run_batched_optimization():\n",
    "    # Settings\n",
    "    Nt = 200\n",
    "    T = 1.0\n",
    "    dt = T / Nt\n",
    "    n_terms = 32\n",
    "    n_samples = 500\n",
    "    d = 2             # param_g needs to be changed accordingly. \n",
    "\n",
    "    params_jax_d = load_model_from_torch(d)\n",
    "    \n",
    "    t_grid = jnp.linspace(0, T, Nt + 1)\n",
    "    basis = bm_kl_basis(t_grid, n_terms) # Precompute basis\n",
    "    \n",
    "    # Generate all random noise at once\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    xi_batch = jax.random.normal(key, (n_samples, n_terms, d))\n",
    "    \n",
    "    print(f\"Compiling and running for {n_samples} samples...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Vectorize the optimizer over the batch of xi\n",
    "    batch_optimizer = vmap(optimize_single_sample_rk4, in_axes=(0, None, None, None))\n",
    "    \n",
    "    final_values = batch_optimizer(xi_batch, params_jax_d, t_grid, dt)\n",
    "    \n",
    "    # Measure time accurately\n",
    "    final_values.block_until_ready()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Total Time: {end_time - start_time:.4f}s\")\n",
    "    \n",
    "    # ---- Statistics ----\n",
    "    lb = jnp.mean(final_values)\n",
    "    se = jnp.std(final_values) / jnp.sqrt(n_samples)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Dual lower bound: {lb:.4f} +/- {1.96 * se:.4f}\")\n",
    "    print(f\"95% lower confidence limit: {lb - 1.96 * se:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "run_batched_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83331683-5d60-4ada-bd4b-5e6cf8c82fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
